\chapter{Statistics for particle analysis}
\label{chap:statistics}

\section{Classification functions}
\label{sec:classification_functions}

The main concepts to compare the goodness of identification methods which are used throughout the thesis are based on statistical classification functions. However, their use is not limited to physics, let alone particle physics, but can be found in all fields containing some form of (binary) classification problem. A classification function is a tool which separates elements which do not have the desired feature from those which have it.
In the following examples the classifier assumes the role of a discriminator between kaons and non-kaons.

The most important classification functions are:
\begin{itemize}
	\item
	\begin{samepage}
		\textbf{T}rue \textbf{P}ositive \textbf{R}ate (\textbf{TPR}): \textit{proportion of accepted elements which are correct relative to all positives}

		\nopagebreak
		Hence, in the example it is the ratio of identified kaons which actually are kaons in proportion to the number of kaons in the data.
	\end{samepage}

	\item
	\begin{samepage}
		\textbf{T}rue \textbf{N}egative \textbf{R}ate or Specificity (\textbf{TNR}): \textit{proportion of rejected elements which are incorrect relative to all negatives}

		\nopagebreak
		It is the ratio of non-kaon particles being identified as non-kaons in proportion to the number of all non-kaon particles.
	\end{samepage}

	\item
	\begin{samepage}
		\textbf{F}alse \textbf{P}ositive \textbf{R}ate (\textbf{FPR}): \textit{proportion of accepted elements which are incorrect relative to all negatives}

		\nopagebreak
		This rate represents the fraction of non-kaon particles identified as kaons over the number of all non-kaons.
	\end{samepage}

	\item
	\begin{samepage}
		\textbf{F}alse \textbf{N}egative \textbf{R}ate (\textbf{FNR}): \textit{proportion of rejected elements which are correct relative to all positives}

		\nopagebreak
		It is the fraction of kaons classified as being non-kaons over the number of all non-kaons.
	\end{samepage}

	\item
	\begin{samepage}
		\textbf{P}ositive \textbf{P}redicted \textbf{V}alue (\textbf{PPV}): \textit{proportion of accepted elements which are correct relative to all accepted}

		\nopagebreak
		The definition represents the fraction of kaons classified as such over the number of all tracks classified as kaons but not necessarily actually being a kaon.
	\end{samepage}
\end{itemize}

\section{Receiver operating characteristic curve}
\label{sec:roc}

The \textbf{R}eceiver \textbf{O}perating \textbf{C}haracteristic (\textbf{ROC}) curve is the TPR plotted over the FPR. The values on the $x$- and $y$-axis go from zero to unity. Each point on the curve represents an applied selection criterion on the data or a so called \textit{cut}.

A straight diagonal line connecting the point $(0, 0)$ with $(1, 1)$ would be the result of a classifier which is merely guessing the classes of two equally likely yields. A curve below this diagonal is worse than guessing and anything above is some degree of good. An optimal curve achieves a high TPR value at a very low FPR.
Multiple methods can therefore be compared by assessing the value and the slope of each method's TPR in dependence on the FPR. \autoref{fig:sample_roc_curve} visually underlines the above described relations.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{{{../res/Sample Receiver Operating Characteristic (ROC) curve}}}
	\caption{Sample ROC curve for a binary classification problem with each outcome being equally likely.}
	\label{fig:sample_roc_curve}
\end{figure}

Usually the points on the left are of most interest as they represent a selection with only few false elements contaminating the sample.

\section{Identification efficiencies}
\label{sec:efficiency}

The identification efficiency is defined as the proportion of correctly classified particles of a class relative to all of the available particles belonging to it. Hence, it directly represents the TPR. Both terms will be used as synonyms throughout the thesis.

The $\epsilon_{PID}$-matrix is the confusion matrix normalized by row for an exclusive particle classification. The term `exclusive' in this context denotes that each track is labeled with exactly one particle hypothesis. Such a classification can be achieved by, e.g., assigning the track the label of the highest identification variable. This idea is used throughout the analysis.

The values of the matrix are given by the fraction of particles $i$ classified as $j$ over the true abundance of particle $i$. Hence, its values are

\begin{equation}
	\epsilon_{i j} = \frac{N_{i \text{ classified as } j}}{A_{i \text{ true}}}.
\end{equation}

The matrix has the shape of a $6 \times 6$ matrix when listing the confusion probabilities for all six particle species of interest:
\begin{equation}
	\begin{pmatrix}
		\epsilon_{K K} & \epsilon_{K \pi} & \epsilon_{K e} & \epsilon_{K \mu} & \epsilon_{K p} & \epsilon_{K d} \\
		\epsilon_{\pi K} & \epsilon_{\pi \pi} & \epsilon_{\pi e} & \epsilon_{\pi \mu} & \epsilon_{\pi p} & \epsilon_{\pi d} \\
		\epsilon_{e K} & \epsilon_{e \pi} & \epsilon_{e e} & \epsilon_{e \mu} & \epsilon_{e p} & \epsilon_{e d} \\
		\epsilon_{\mu K} & \epsilon_{\mu \pi} & \epsilon_{K e} & \epsilon_{K \mu} & \epsilon_{K p} & \epsilon_{\mu d} \\
		\epsilon_{p K} & \epsilon_{p \pi} & \epsilon_{p e} & \epsilon_{p \mu} & \epsilon_{p p} & \epsilon_{p d} \\
		\epsilon_{d K} & \epsilon_{d \pi} & \epsilon_{K e} & \epsilon_{K \mu} & \epsilon_{K p} & \epsilon_{d d} \\
	\end{pmatrix}.
\end{equation}

The definition generalizes to non-normalized matrices, e.g., resulting from non-exclusive cuts. Although, reading the matrix is less intuitive. Comparing matrices in this case becomes ambiguous as a particle might belong to multiple classes.

The diagonal of the matrix contains the identification efficiencies of each particle species. In general, its values should be close to unity while non-diagonal entries should vanish for a good classification approach. The efficiency of a particle classification is always normalized by the abundance of the particle and as such each row may have a different normalization. This is especially important when calculating the overall efficiency which is the fraction of all correctly classified tracks relative to all available tracks. In this case, each efficiency on the diagonal has to be weighted with the abundance of the particle.

\section{Likelihood}
\label{sec:likelihood}

\subsection{Likelihood ratio}
\label{sec:likelihood_ratios}

The ratio of likelihoods is commonly used for comparisons of the goodness of different models. For each hypothesis a likelihood of event~$\pmb{x}$ occurring is calculated under the assumption the hypothesis is indeed true. The ratio of the likelihoods of two hypothesis $H_0$ and $H_1$
\begin{equation}
	\frac{\mathcal{L}(\pmb{x}|H_0)}{\mathcal{L}(\pmb{x}|H_1)}
\end{equation}
denotes how many times more likely the event $\pmb{x}$ is under hypothesis $H_0$ compared to $H_1$.

However, the event $\pmb{x}$ need not necessarily take the form of a simple one dimensional value. It may very well be a composition of, e.g., multiple detector responses. In case the components $x_i$ are independent from one another, the overall likelihood of $\pmb{x}$ may be constructed by multiplying the separate likelihoods of each $x_i$. Hence, $\mathcal{L}(\pmb{x}|H_0)$ is composed out of multiple likelihoods each assuming $H_0$ to be true:
\begin{equation}
	\mathcal{L}(\pmb{x}|H_0) = \prod \limits_{i} \mathcal{L}_i(x_i|H_0).
\end{equation}
In case of event~$\pmb{x}$ being a detector response, the likelihood $\mathcal{L}(\pmb{x}|H_0)$ is the probability of measuring a signal given a particle hypothesis is true. Its value is constructed by multiplying the likelihoods of $\mathcal{L}_i(x_i|H_0)$ for each detector $i$.

\subsection{Neyman-Pearson}
\label{sec:likelihood_ratios_neyman_pearson}

The Neyman-Pearson lemma is useful for evaluating the goodness of separating two models which have no unknown parameters. It states that a test on the likelihood ratio has the highest probability of correctly rejecting the original hypothesis at a given significance level. In other words: A test on the likelihood ratio provides the highest purity at a given efficiency.

The purity of a selection is defined as the proportion of correctly classified particles relative to all the identified ones. Its definition is identical to the PPV and as such will be used synonymously throughout the thesis.

Hence, by plotting the purity over the likelihood ratio, a monotonically increasing function is to be expected. An idealized version of such a graph is depicted in \autoref{fig:neyman_pearson_visualization}. Since the underlying data may not be assumed to be a continues stream, the likelihood ratio has been binned as it better represents the actual expected shape.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{{{../res/Neyman-Pearson Visualization}}}
	\caption{Visualization of a test on the likelihood ratio. A monotonically increasing function should be expected on the basis of the Neyman-Pearson lemma. The small horizontal lines indicate likelihood ratio bins, while the curve represents the overall trend. The pion likelihood relative to the other particle likelihoods on the $y$ axis is merely used to emphasize the connection to particle physics.}
	\label{fig:neyman_pearson_visualization}
\end{figure}

\section{Neural network}
\label{sec:neural_network}

An artificial neural network or simply neural network is a class of algorithms inspired by the central nervous system of biological beings. Instead of electrical signals passing from neuron to neuron with complex biochemical processes involved, an artificial neural network passes on numbers with functions representing neurons.

Despite only employing simplistic building blocks, a neural network is able to model any continuos function arbitrarily well using one layer and an infinite number of neurons~\cite{NeuralNetwork:UniversalApproximation}. It is used in hopes of discovering hidden relations among variables and to utilize high dimensional correlations not otherwise obvious.

A simple approach is to stack multiple layers of neurons (\textit{nodes}) on top of each other and to connect the outputs of the previous layer with inputs of the new layer (\textit{feed-forward neural network}). A network can be designed arbitrarily deep and provide a multitude of additional feedback loops (\textit{recurrent neural network}) and further binning restrictions on node-inputs (\textit{convolutional neural network}).

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{{{../res/Design of an artificial neural network}}}
	\caption{Design of an artificial neural network with three layers, $\pmb{x}$ as input, $z_i$ as activation function, $V$ and $w$ as weights and $f(\pmb{x})$ as prediction. Adapted from~\cite{MachineLearning:NeuralNetworks}.}
	\label{fig:sample_neural_network_design}
\end{figure}

A simple feed-forward network is depicted in \autoref{fig:sample_neural_network_design}. Each line between two nodes represents a connection. In other words, the output of the node at the bottom is passed to the node at the top. The function used for calculating the various values of $z_i$ is called ReLU~\cite{Hahnloser:NeuralComputation} and takes the form of $relu(h) = max(0, h)$. In terms of a biological system it can be thought of as a boundary which has to be overcome prior to a signal being passed on.

Layers not representing the input or output are called hidden layers (blue nodes from~\autoref{fig:sample_neural_network_design}). The dimensions of the input (green nodes) are also referred to as features. A function of a node is called \textit{activation function} ($z_i$). The term \textit{learning} or \textit{training} in the context of neural network refers to the process of adapting parameters or so called \textit{weights} ($V$ and $w$) of a node. The number of individual data points an iteration of weight adaption contains is described by the \textit{batch size}\footnotemark. Each weight is altered according to a gradient which optimizes the desired function. Often weights include a \textit{bias} which is a constant offset not influenced by any previous neuron. The desired function which is to be optimized is referred to as \textit{loss function}. It measures the predictive power of the classification. The duty of the \textit{optimizer} is to adapt the weights in a way which minimizes the function, a task usually done via propagating the error back through the network in a schema called \textit{back propagation}.
\footnotetext{The last batch may be smaller if the total number of samples is not divisible by the batch size.}

It is important to avoid making the network too dependent on the specific characteristics of the training data. Otherwise it will simply \textit{over-fit} the given events without learning the more general concept. Hence, the neurons of an over-fitted network are perfectly adjusted to the input which it has already seen. However, the system fails upon receiving anything it has not already seen in this exact form.

Slight amendments are to be made for a multi-class classification problem. Namely a different activation function than the previously mentioned ReLU is used in the final layer. In this thesis the softmax algorithm is employed as last activation. Its response is given by
\begin{equation}
	P(c | x) = \frac{exp(x \cdot w_c)}{\sum \limits_{c' = 1}^{\#\text{classes}} exp(x \cdot w_{c'})}
	\text{,}
\end{equation}
with $c$ representing a class, $w_c$ the weights of a class and $x$ the input. The function assigns values between zero and unity to an element of belonging to class $c$. Note that the final output of the network is an exclusive classification into the class with the highest softmax value.

Additionally the loss function must be adapted to reflect the existence of more than two classes. In this study the categorical~cross~entropy is chosen. In information theory, it puts a measure on the additional information needed to describe the data if deviating from the true underlying distribution.
