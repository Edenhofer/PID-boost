\chapter{Statistics for particle analysis}
\label{chap:statistics}

\section{Classification functions}
\label{sec:classification_functions}

A main part of identifying particles is to examine statistical measures. The main concepts used throughout the thesis heavily relies on such values to compare the goodness of a identification method. However their use is not limited to physics, let alone particle physics, but spans over all field containing some form of (binary) classification problems.
In the following explanatory paragraphs I will assume that we are about to identify kaons in a set of data containing a multitude of alternative particles.

The most important classification functions are:
\begin{itemize}
    \item \textbf{T}rue \textbf{P}ositive \textbf{R}ate (\textbf{TPR}): \textit{Rate of accepted elements which are correct}

    \nobreak
    Hence the ratio of identified kaons which actually were kaons in proportion to the number of kaons in the data.

    \item \textbf{T}rue \textbf{N}egative \textbf{R}ate (\textbf{TNR}): \textit{Rate of rejected elements which are incorrect}

    \nobreak
    In our example this would translate to the ratio of non-kaon particles being identified as being non-kaons in proportion to the number of all non-kaon particles.

    \item \textbf{F}alse \textbf{P}ositive \textbf{R}ate (\textbf{FPR}): \textit{Rate of accepted elements which are incorrect}

    \nobreak
    This would translate to the fraction of non-kaon particles identified as kaons over the number of all non-kaons.

    \item \textbf{F}alse \textbf{N}egative \textbf{R}ate (\textbf{FNR}): \textit{Rate of rejected elements which are correct}

    \nobreak
    Using our kaon sample once more; this would represent the fraction of kaons classified as being non-kaons over the number of all non-kaons.
\end{itemize}

In abstract but more general terms; the above description can be summarized as seen in table~\ref{tab:classification_guidelines}.

\vspace{2em}
\begin{table}
    \centering
    \begin{tabular}{l|ll}
        Veracity & True = correct & False = incorrect \\
        Identification & Positive = accepted & Negative = rejected
    \end{tabular}
    \caption{Guidelines for understanding the meaning of a classification function.}
    \label{tab:classification_guidelines}
\end{table}

\section{Receiver operating characteristic}
\label{sec:roc}

The \textbf{R}eceiver \textbf{O}perating \textbf{C}haracteristic (\textbf{ROC}) curve is the TPR plotted over the FPR. As such the values on the $x$ and $y$ axis go from $0$ to $1$. Each point on the curve represent an applied selection criterion on the data.

On a set of data with two equally likely yields a straight diagonal line connecting the point $(0, 0)$ with $(1, 1)$ would represent plain guessing. A curve below that would be worse and anything above, is some degree of good. An optimal curve would achieve a high TPR value at a very low FPR.
Multiple methods can therefore be compared by assessing how steep each methods TPR is rising relative to the FPR. The graph~\ref{fig:sample_roc_curve} visually underlines the above described relations.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{{{../res/Sample Receiver Operating Characteristic (ROC) curve}}}
    \caption{Sample ROC curve for a binary classification problem with each outcome being equally likely.}
    \label{fig:sample_roc_curve}
\end{figure}
