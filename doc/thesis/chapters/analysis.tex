\chapter{Analysis}
\label{chap:analysis}

\section{Data sample}
\label{sec:data_sample}

As outlined in \autoref{subsec:particle_identification}, the main objective of particle identification at Belle~\RN{2} is to identifying tracks of kaons, pions, electrons, muons, protons and deuterons. In order to validate a particle identification approach and to make sure it is behaving as expected it is standard procedure to measure the performance on Monte Carlo simulated data. For the purpose of simulating the production and decay of the $\Upsilon(4S)$ the software framework EvtGen was used. The subsequent generic decay was simulated using generic decay files provided by the Belle~\RN{2} software. After the simulation of the particle and their various properties, the detector responses are emulated. Hits in the various detector components are simulated and at last the veracity of a track identification is calculated. For obvious reasons the process of matching the identification with the truth value is not possible for real data, however for testing purposes it is a valuable tool to measure and compare the performance of a new approach.

Throughout this thesis several decays will be discussed, most notably the decay of the charged $B$ mesons. $\Upsilon(4S)$ decays in $(51.4 \pm 0.6) \%$ of cases into the charged $B^+$ and $B^-$. Therefore it represents a good sample of the overall to be expected particle species. Observations seen in this generic charged decay were validated using data of the `mixed' decay into $B^0$ and $\bar{B}^0$ which has a branching ratio of $(48.6 \pm 0.6) \%$. Both samples are generic decays and not specific to one analysis. A complete list of possible decay-strings as well as the above mentioned branching fractions may be found in~\cite{Patrignani:2016xqp}.

Additional a simulated decay of the $B^+ B^-$ with non-generic properties was generated. The properties are outlined in \autoref{tab:simulated_decay}. It allowed for fast processing of tests due to its simplicity and helped in differentiating between decay specific observations and generic ones.

\begin{table}[ht]
	\centering
	\begin{tabular}{ll}
		$\Upsilon(4S) \rightarrow B^+ B^-$ & $1.$ \\
		\quad $B^+ \rightarrow \mu^+ \nu_{\mu} \gamma$ & $1.$ \\
		\quad $B^- \rightarrow \pi^- D^0$ & $1.$ \\
		\qquad $D^0 \rightarrow K^- \pi^+$ & $0.2$ \\
		\qquad 	$D^0 \rightarrow K^- \pi^+ \pi^0$ & $0.2$ \\
		\qquad 	$D^0 \rightarrow K^- \pi^+ \pi^+ \pi^-$ & $0.2$ \\
		\qquad 	$D^0 \rightarrow K^- K^+$ & $0.2$ \\
		\qquad 	$D^0 \rightarrow \pi^+ \pi^0$ & $0.2$ \\
	\end{tabular}
	\caption{Simulated non-generic decay of the $\Upsilon(4S)$ with charge conjugated decays implied.}
	\label{tab:simulated_decay}
\end{table}

As seen in \autoref{fig:true_particle_abundance} the decays are dominated by kaons and pions. However the overall distribution is much more peaked for the charged generic decay. Furthermore the non-generic sample decay features a feature a lot more $\mu^+$ relatively speaking in comparisons charged generic decay. The mixed generic decay has been disregarded for this plot due to its distinct similarity to the charged decay.

\begin{figure}[ht]
	\centering
	\subcaptionbox{Charged\label{fig:charged_decay_true_particle_abundance}}{
		\includegraphics[width=0.43\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/General Purpose Statistics: True Particle Abundances in the K+-Data}}}
	}
	\hspace{2em}
	\subcaptionbox{Sample\label{fig:sample_decay_true_particle_abundance}}{
		\includegraphics[width=0.43\textwidth,height=\textheight,keepaspectratio]{{{../res/sample/General Purpose Statistics: True Particle Abundances in the K+-Data}}}
	}
	\caption{True particle abundance in various simulated decays with particle on the $x$-axis sorted by their frequency. \textit{NaN} stands for an invalid translation\protect\footnotemark from the particles' PDG code to an actual particle.}
	\label{fig:true_particle_abundance}
\end{figure}
\footnotetext{The error occurs due to the PDG code in the ROOT file being saved as \lstinline|float32| but some particle's code exceed the memory limit of $32$-bits. Notably, this effects the deuteron as well as its anti-particles.}

The generic decay files used in the following discussion (charged for exploration, mixed for validation) each feature about $100,000$ initial $B \bar{B}$ events and about ten times as many identified tracks. The non-generic decay contains $10,000$ initial events and about $40,000$ tracks.

If not specifically otherwise stated it shall be assumed that the graphs and visuals in the following paragraphs are based on the data from the generic charged decay.

\section{Particle identification variables}
\label{sec:pid_variables}

\subsection{Legacy PID}
\label{subsec:pid_variables_legacy_pid}

The current particle identification approach consists of variables calculated via ratios of likelihoods. A particle identification is performed by applying a selection, also called \textit{cut} on the variables.

As of \formatdate{28}{06}{2018} the current approach is to take the likelihood of the desired particle and divide it by itself plus the likelihood of the pion. To construct the ID of the pion the kaon likelihood is used as second term in the denominator. \autoref{tab:legacy_particleid_variables} shows the definition of the ID for each of the six particle species of interest. In the future it will be replaced by the pidProbability approach, described in \autoref{subsec:pid_variables_pidProbability}.

\begin{table}[ht]
	\centering
	\begin{tabular}{l|l}
		pionID & $\mathcal{L}_{\pi} / (\mathcal{L}_{\pi} + \mathcal{L}_{K})$ \\
		kaonID & $\mathcal{L}_{K} / (\mathcal{L}_{K} +\mathcal{L}_{\pi})$ \\
		protonID & $\mathcal{L}_{p} / (\mathcal{L}_{p} +\mathcal{L}_{\pi})$ \\
		electronID & $\mathcal{L}_{e} / (\mathcal{L}_{e} +\mathcal{L}_{\pi})$ \\
		muonID & $\mathcal{L}_{\mu} / (\mathcal{L}_{\mu} +\mathcal{L}_{\pi})$ \\
		deuteronID & $\mathcal{L}_{d} / (\mathcal{L}_{d} +\mathcal{L}_{\pi})$
	\end{tabular}
	\caption{Definition of the ParticleID variables currently used by default for particle identification.}
	\label{tab:legacy_particleid_variables}
\end{table}

Its identification rate for pions and kaons is therefore rather good as it able to properly differentiate both classes. However it has obvious limitations in identifying rare particles as the fraction will always be dominated by the high abundance of the pion. The difference in the TPR for identifying the kaon and the electron can be seen in \autoref{fig:legacy_pid_particle_identification}. Not only is does the TPR take a less steep ascent for the electron identification the PPV is also consistently lower.

\begin{figure}[ht]
	\centering
	\subcaptionbox{Kaon identification\label{fig:legacy_pid_kaon_identification}}{
		\includegraphics[width=0.4\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/Particle ID Approach: K Identification}}}
	}
	\hspace{2em}
	\subcaptionbox{Electron identification\label{fig:legacy_pid_electron_identification}}{
		\includegraphics[width=0.4\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/Particle ID Approach: e Identification}}}
	}
	\caption{Particle identification rates for the kaon and electron using the legacy PID approach, each showing the True Positive Rate (ROC curve) and the Positive Predicted Value depending on the False Positive Rate.}
	\label{fig:legacy_pid_particle_identification}
\end{figure}

This unbalanced classification is further emphasized by analyzing the identification efficiencies. The matrix shown in \autoref{fig:legacy_pid_epsilon_pid} clearly shows the bias of the classification towards kaons and pions.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth,height=0.3\textheight,keepaspectratio]{{{../res/charged 01/Particle ID Approach: Heatmap of epsilonPID Matrix for an exclusive Cut}}}
	\caption{Matrix of $\epsilon$ values for the legacy particle ID approach.}
	\label{fig:legacy_pid_epsilon_pid}
\end{figure}

\subsection{pidProbability}
\label{subsec:pid_variables_pidProbability}

The pidProbability approach is the upcoming approach which will be used as new default variable on which to select particle samples. Instead of having the likelihood of the pion fixed in the denominator of every particle's id it is now replaced by the sum over all the other particles. Hence the pidProbability of the kaon is now represented by the likelihood of the kaon divided by sum over all the likelihoods of every other particle including the kaon itself. The complete list of the definition can be seen in \autoref{tab:pidProbability_variables}.

\begin{table}[ht]
	\centering
	\begin{tabular}{l|l}
		pidProbabilityPion & $\mathcal{L}_{\pi} / \mathcal{L}_{all}$ \\
		pidProbabilityKaon & $\mathcal{L}_{K} / \mathcal{L}_{all}$ \\
		pidProbabilityProton & $\mathcal{L}_{p} / \mathcal{L}_{all}$ \\
		pidProbabilityElectron & $\mathcal{L}_{e} / \mathcal{L}_{all}$ \\
		pidProbabilityMuon & $\mathcal{L}_{\mu} / \mathcal{L}_{all}$ \\
		pidProbabilityDeuteron & $\mathcal{L}_{d} / \mathcal{L}_{all}$ \\
		\hline
		\multicolumn{2}{c}{$\mathcal{L}_{all} = \sum \limits_{x \in {\pi, K, p, e, \mu, d}} \mathcal{L}_{x}$}
	\end{tabular}
	\caption{Definition of the pidProbability variables which will be used by default for particle identification in the future.}
	\label{tab:pidProbability_variables}
\end{table}

The approach is significantly less prone to just differentiating between kaons and pions but instead has a more balanced classification as can be seen in \autoref{fig:pidProbability_epsilon_pid}. This newly gained precision for less abundance particles however comes at the cost of worsening the classification with a very high abundance like the pion.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth,height=0.3\textheight,keepaspectratio]{{{../res/charged 01/pidProbability Approach: Heatmap of epsilonPID Matrix for an exclusive Cut}}}
	\caption{Matrix of $\epsilon$ values for the pidProbability approach.}
	\label{fig:pidProbability_epsilon_pid}
\end{figure}

\subsection{pidProbability goodness}
\label{subsec:pid_variables_pidProbability_goodness}

In order to make sure the pidProbability variable is properly defined, one may validate its behavior using the Neyman-Pearson lemma outlined in \autoref{subsec:likelihood_ratios_neyman_pearson}. The lemma states that the highest purity for a given efficiency is to be expected for each selection on the likelihood ratio as seen in \autoref{fig:neyman_pearson_visualization}.

For the following discussion the data has been sampled into $10$ bins of equal height as to provide a balanced statistics for each bin. The error in a bin is given by a vertical line. It is calculated via gaussian error propagation under the assumption that the counting of the events follow a Poisson distribution. Thereby we assume that the number of desired particles in a bin and the number of undesired particles are independent. The error will be underestimated for a purity of $0$ and $1$.

When applying the previously discussed approach to the data the goodness of the likelihoods can be validated as seen in \autoref{fig:likelihood_ratio_kaon_all}. The purity of the kaon sample in the bins increases with a stricter likelihood cut and the uncertainty is low due to the high statistics. However \autoref{fig:likelihood_ratio_proton_all} paints a completely different picture. The purity peaks at an likelihood ratio of about $0.35$, while the following values are far lower. However this effect is unique to the proton and can not be observed in any other of the six particle species of interest. On one hand this means the likelihoods for the kaon, pion, electron, muon and deuteron are properly defined and actually behave like probabilities. On the other hand it also reveals a flaw in the calculation of the proton likelihood. Thankfully the proton does not play a significant role in the analysis done in the following sections but nevertheless it is important to keep in mind that identifying protons via their likelihood (likelihood ratio) has inherent flaws.

\begin{figure}[ht]
	\centering
	\subcaptionbox{Kaon\label{fig:likelihood_ratio_kaon_all}}{
		\includegraphics[width=0.4\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/pidProbability Approach: Relative K Abundance in Likelihood Ratio Bins for ALL detector}}}
	}
	\hspace{2em}
	\subcaptionbox{Proton\label{fig:likelihood_ratio_proton_all}}{
		\includegraphics[width=0.4\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/pidProbability Approach: Relative p Abundance in Likelihood Ratio Bins for ALL detector}}}
	}
	\caption{Relative abundance (purity) of various particle samples in likelihood ratio bins.}
	\label{fig:likelihood_ratio_all}
\end{figure}

In order to understand the observed effect in \autoref{fig:likelihood_ratio_proton_all} it is important to pin down its cause. Since the likelihoods are values which are returned by each detector one natural conclusion might be that it is caused by one poorly defined detector response. \autoref{fig:likelihood_ratio_proton_by_detector} shows the relative abundance of the proton in likelihood ratio bins for various detector components.

\begin{figure}[ht]
	\centering
	\subcaptionbox{SVD\label{fig:likelihood_ratio_proton_svd}}{
		\includegraphics[width=0.3\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/pidProbability Approach: Relative K Abundance in Likelihood Ratio Bins for SVD detector}}}
	}
	\hspace{0.5em}
	\subcaptionbox{CDC\label{fig:likelihood_ratio_proton_cdc}}{
		\includegraphics[width=0.3\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/pidProbability Approach: Relative p Abundance in Likelihood Ratio Bins for CDC detector}}}
	}
	\hspace{0.5em}
	\subcaptionbox{TOP\label{fig:likelihood_ratio_proton_top}}{
		\includegraphics[width=0.3\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/pidProbability Approach: Relative p Abundance in Likelihood Ratio Bins for TOP detector}}}
	}

	\subcaptionbox{ARICH\label{fig:likelihood_ratio_proton_arich}}{
		\includegraphics[width=0.3\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/pidProbability Approach: Relative p Abundance in Likelihood Ratio Bins for ARICH detector}}}
	}
	\hspace{0.5em}
	\subcaptionbox{ECL\label{fig:likelihood_ratio_proton_ecl}}{
		\includegraphics[width=0.3\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/pidProbability Approach: Relative p Abundance in Likelihood Ratio Bins for ECL detector}}}
	}
	\hspace{0.5em}
	\subcaptionbox{KLM\label{fig:likelihood_ratio_proton_klm}}{
		\includegraphics[width=0.3\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/pidProbability Approach: Relative p Abundance in Likelihood Ratio Bins for KLM detector}}}
	}

	\caption{Relative abundance (purity) of the proton in likelihood ratio bins for all available detectors.}
	\label{fig:likelihood_ratio_proton_by_detector}
\end{figure}

The response of the SVD is in perfect agreement with the expectation, the CDC however shows the same kink merely shifted a little to the left and the TOP is in agreement again. The ARICH, ECL and KLM either have a bad statistics and or the likelihoods not filling the whole range and underestimate the error at critical points close to a purity of $0$. Hence the ECL seems like the only viable cause of the unexpected kink in the proton purity over likelihood ratio plot. The observed shift to the left is due to the likelihood of a single detector in general being lower than the  likelihood of all detectors combined.

As a further clue the $p_t$ dependency in the CDC detector was analysed as depicted in \autoref{fig:likelihood_ratio_proton_by_pt}. As sampling method three equal height bins were chosen, as using rather vew bins with the same overall number of particles in each provides a good statistic and allows for fair comparisons. Analyzing the picture it becomes obvious that especially low to medium transverse momentum protons constituent to the effect.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textheight,keepaspectratio]{{{../res/charged 01/pidProbability Approach: Relative p Abundance in Likelihood Ratio Bins for CDC detector for equal size pt bins}}}
	\caption{Relative abundance of the proton in likelihood ratio bins for different transverse momentum ranges in units of $GeV/c$ for the CDC detector.}
	\label{fig:likelihood_ratio_proton_by_pt}
\end{figure}

As a further information it is important to bear the locations of the detector components in mind. Of special interest in the discussed case is the end of the TOP detector, at roughly $- 60^{\circ}$ relative to the beampipe (see~\autoref{fig:belle2_detector_design_white_paper}). At this point the particle identification gets notably different. However as this effects all tracks equally it cancels out in comparisons.

In conclusion the likelihood of the proton given by the CDC detector should be seen with a healthy skepticism and not be taken for granted as it does not behave as a likelihood. Further analysis needs to take place in order to pin down the actual cause.

\section{Bayesian approach}
\label{sec:bayesian_approach}

\subsection{Simple Bayes}
\label{subsec:bayesian_approach_simple_bayes}

The goal of a Bayesian approach is to weight the particles' probability by their abundance in the sample. This process increases the likelihood of a particle being identified as belonging to a group with a higher abundance and decreases the likelihood of being identified as belonging to a group with a lower abundance. Bayes law provides the mathematical foundation as is given by
\begin{equation*}
	\displaystyle P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
	\qquad
	\text{e.g.} \quad P(e|Signal) = \frac{P(Signal|e) \cdot P(e)}{P(Signal)}.
\end{equation*}
The variable $P(e|Signal)$ is the probabilities of the track being from an electron given that $Signal$ was measured. The part of most interest in the equation is the a prior probability $P(A)$, respectively $P(e)$. As a first simple approach this variable is now dependant on the absolute abundance of electrons. The probabilities of the detecting the signal can be modeled using
\begin{equation*}
	P(Signal) = \sum \limits_{x \in {\pi, K, p, e, \mu, d}} P(Signal|x) \cdot P(x)
\end{equation*}
with $P(Signal|x)$ being the previously discussed likelihoods of the signal using the particle hypothesis $x$. Note that the normalization of $P(x)$ cancels out as it occurs in both nominator and denominator. Therefore, it makes no difference if in the actual calculation the value of the abundance is used directly.

The absolute particle abundance of a sample taken from the Monte Carlo simulation of the charged decay of the $B$-mesons can be seen in \autoref{fig:charged_decay_true_particle_abundance}. In this example the bias towards pions and kaons can be clearly observed.

The approach depends on the detector yielding decay-agnostic results. Hence the detector shall be assumed to always output the likelihood of measuring the received signal given a specific particle hypothesis regardless of prior probabilities. Furthermore the approach assumes a bias towards one or a few particles in the data since otherwise the a priori probabilities would be flat and would have no effect.
Thankfully both of those hypothesis are fulfilled in the real world: The detector can be assumed to behave independently of the relative particle abundance and the measured data usually shows a clear predominance of one or a few particle species. This is not surprising in itself as the branching fractions are not equally distributed. Dictated by the laws of physics one particle species might be produced more frequently.

\subsection{Univariate Bayes}
\label{subsec:bayesian_approach_univariate_bayes}

The univariate Bayesian approach adds a further dependency in the form of one detector variable to the a priori probabilities of the Bayesian approach. Hence instead of having a priori probability which depends only on the particle abundance, the univariate approach additionally varies the value depending on e.g. the transverse momentum or the angle between beampipe and track $\Theta$. Those two variables make the most sense as they play a significant role in the track fitting process and are dominant factors for the particle classification process outlined in~\autoref{subsec:particle_identification}.

\begin{figure}[ht]
	\centering
	\subcaptionbox{$p_t$\label{fig:univariate_bayes_priors_by_pt}}{
		\includegraphics[width=0.4\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/Univariate Bayesian Approach: K Spectra Ratios Relative to pi for pt bins}}}
	}
	\hspace{2em}
	\subcaptionbox{$\cos(\Theta)$\label{fig:univariate_bayes_priors_by_costheta}}{
		\includegraphics[width=0.4\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/Univariate Bayesian Approach: K Spectra Ratios Relative to pi for cos(Theta) bins}}}
	}
	\caption{Kaon abundance relative to the pion abundance in equal height bins of $p_t$ and $\cos(\Theta)$ with horizontal lines indicating the size of each bin.}
	\label{fig:univariate_bayes_priors}
\end{figure}

The relative abundance of one particle in comparison to the pion is shown in \autoref{fig:univariate_bayes_priors}. Equal height bins were chosen to enforce a good statistics across all bins. It is important that the relations between particle abundances are correct, hence the normalization via the pion abundance, the absolute value however does not matter. The graphs underline the motivation for using such an approach as the abundance varies between different bins. Especially the dependency on $p_t$ revealed drastic changes in the relative frequency of the kaon.

Due to its additional variable the univariate Bayesian approach is able to adapt to the underlying data a little better as it does not fix the abundance of particles in general as the simple Bayesian approach does.

\subsection{Multivariate Bayes}
\label{subsec:bayesian_approach_multivariate_bayes}

The multivariate approach extends the univariate one by further increasing the number of free parameters the a priori probabilities may depends on. As previously mentioned both $p_t$ and $\cos(\Theta)$ make for an excellence choice as default variables for said dependency. For certain pockets those variables provide a good separation between different particle species.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textheight,keepaspectratio]{{{../res/charged 01/Multivariate Bayesian Approach: Multi-axes Histogram of pt, cosTheta}}}
	\caption{Two dimensional scatter plot with tracks represented as opaque points depending on $p_t$ and $\cos(\Theta)$ and the color encoding the particle species.}
	\label{fig:multivariate_bayes_multi_axis_histogram_by_pt_by_costheta}
\end{figure}

\autoref{fig:multivariate_bayes_multi_axis_histogram_by_pt_by_costheta}~shows tracks as transparent dots scattered across the plane, accompanied by two histograms showing the distribution in transverse momentum and the cosine of the angle between beampipe and track. It is important to note the sickle shape of the distribution. Higher cosine values are a little preferred over lower ones due to the asymmetry of the beams and angles of about $90^{\circ}$ ($\cos(\Theta) = 0$) correspond to the highest transverse momentums. Furthermore a slight separation can be seen between the outermost green sickle and the slightly blueish middle sickle. Intuitively this makes sense since the production of e.g. a proton consumes a lot of phase space which in turn reduces the amount of momentum it can carry. Vice verse a pion is relatively speaking light and therefore may carry a lot of momentum (not necessarily transverse momentum though). Obviously this argumentation disregards the fact that particles are not created one by one but rather in a complex decay and the phase space is distributed across all daughters. Nevertheless it illustrates the observed effect quite nicely.

Allowing the a priori probability to depend on those measurements enables it to pick the best priors according to the Monte Carlo information for each combination of variables. In order to achieve this the variable $p_t$ and $\cos(\Theta)$ were distributed across equal height bins and the relative abundance of each particle relative to the pion were calculated for every combination of $p_t$ and $\cos(\Theta)$ bin.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth,height=0.5\textheight,keepaspectratio]{{{../res/charged 01/Multivariate Bayesian Approach: K Spectra Ratios Relative to pi}}}
	\caption{Relative kaon abundance in equal height transverse momentum and cosine of $\Theta$ bins.}
	\label{fig:multivariate_bayes_k_spectra_relative_to_pi}
\end{figure}

The abundance of kaons relative to the abundance of pions can be seen in~\autoref{fig:multivariate_bayes_k_spectra_relative_to_pi}. The graph emphasizes the fact that particles abundances are unevenly distributed across the $p_t$-, $\cos(\Theta)$-plane and as such this additional information may be used to better classify the particles.

\subsection{Comparisons}
\label{subsec:bayesian_approach_comparisons}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{{{../res/charged 01/Diff Statistics: K Identification via PID, via simple Bayes}}}
	\caption{Comparisons of PPV, TPR and FPR for simple Bayes and legacy PID approach for identifying kaons. The upper graph shows both rates of each approach separately using different colors, while the lower visualizes the ratio between the PPV's respectively the TPR's.}
	\label{fig:diff_stats_K_identification_via_pid_via_simple_bayes}
\end{figure}

\autoref{fig:diff_stats_K_identification_via_pid_via_simple_bayes} shows a comparison of the \textbf{legacy PID} approach to the discussed \textbf{simple Bayes}ian one for identifying kaons. Using simple Bayes for identifying particles yields a very high positive predicted values even for low false positive rates as the introduced bias in the particle classification can utilize the unbalanced abundances. Furthermore the true positive rate of the Bayesian approach has a much steeper increase in comparison to the legacy PID's one. However it levels of quicker, indicating that it fails to identify a few particles even at a high false positive rate.
The described effect can be seen for every stable particle with an ID and is not limited to the kaon. However the comparison of the kaon identification is a rather conservative choice as the legacy PID approach favors high abundance particles. Particles like the electron or muon perform consistently better using the new approach.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth,height=0.3\textheight,keepaspectratio]{{{../res/charged 01/Diff Heatmap: Heatmap of epsilonPID Matrix for an exclusive Cut via PID, via simple Bayes}}}
	\caption{Comparisons of the row-wised normed confusion matrix for simple Bayes and the legacy PID approach.}
	\label{fig:diff_heatmap_via_pid_via_simple_bayes}
\end{figure}

The improvements in the identification efficiencies are less obvious for an exclusive cut on the identifying variables. However in general the simple Bayesian approach is less prone to confusing particle with one another as seen in~\autoref{fig:diff_heatmap_via_pid_via_simple_bayes}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth,height=0.3\textheight,keepaspectratio]{{{../res/charged 01/Diff Heatmap: Heatmap of epsilonPID Matrix for an exclusive Cut via pidProbability, via simple Bayes}}}
	\caption{Comparisons of the row-wised normed confusion matrix for pidProbability and simple Bayes.}
	\label{fig:diff_heatmap_via_pidProbability_via_simple_bayes}
\end{figure}

In comparison to the \textbf{pidProbability} variable the differences are less pronounced as seen in~\autoref{fig:diff_heatmap_via_pidProbability_via_simple_bayes}. At this point it becomes a question of the desired goal. Surely the pion identification is boosted but only at the cost of loosing efficiency for the muon. At the same time the efficiencies for the kaon, electron and proton identification stayed about the same. However overall the approach is better since kaons are present in a higher abundance and therefore technically more particles are correctly classified as seen in~\autoref{fig:diff_abundances_via_pid_via_pidprobability_via_simple_bayes}. Especially the Bayesian approach shines in when comparing the true and predicted particle abundances as the information was built into its design.

\begin{figure}[ht]
	\centering
	\subcaptionbox{PID, simple Bayes\label{fig:diff_abundances_via_pid_via_simple_bayes}}{
		\includegraphics[width=0.4\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/Diff Abundances: Particle Abundances in the K+-Data via PID, via simple Bayes}}}
	}
	\hspace{2em}
	\subcaptionbox{pidProbability, simple Bayes\label{fig:diff_abundances_via_pid_via_simple_bayes}}{
		\includegraphics[width=0.4\textwidth,height=\textheight,keepaspectratio]{{{../res/charged 01/Diff Abundances: Particle Abundances in the K+-Data via pidProbability, via simple Bayes}}}
	}
	\caption{Counting from the top the first line of each approach (color coded) represents the abundance according to the approach, while the lower line represents the number of correctly classified particles. The particle classification hereby is exclusive.}
	\label{fig:diff_abundances_via_pid_via_pidprobability_via_simple_bayes}
\end{figure}

Introducing more degrees of freedom in the form of the \textbf{univariate Bayes}ian approach further extends the existing strengths of the simple Bayesian approach while compensating some of the downsides. Using the kaon again as conservative sample the identification improves slightly as seen in~\autoref{fig:diff_stats_K_identification_via_simple_bayes_via_univariate_bayes}. Although slightly less particles are classified overall which can be seen via the ratio of the TPRs being below $1.$, a boost of the rate of correctly classified particles more than compensates for the slight loss. The efficiencies for each particle identification further manifests this view although
Overall its behavior is very similar to the simple Bayesian one and a comparison between univariate Bayes and pidProbability respectively the legacy PID approach would therefore be redundant.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{{{../res/charged 01/Diff Statistics: K Identification via simple Bayes, via univariate Bayes}}}
	\caption{Comparisons of PPV, TPR and FPR for simple Bayes and univariate Bayes for identifying kaons. The tail of the PPV ratio is due the ever decreasing PPV in general and does not necessarily mean it that superior.}
	\label{fig:diff_stats_K_identification_via_simple_bayes_via_univariate_bayes}
\end{figure}

Lastly the \textbf{multivariate Bayes}ian approach was tested. Its TPR and PPV are for every particle species in the sample, neglecting effects below a maximal relative changes of $2 \%$, identical to the univariate approach. At this point it is important to emphasize that the new dependency improved results rather than worsen them; Negative effects in TPR and PPV were always smaller than $0.3 \cdot 10^{-3}$ while the maximal change of $2 \%$ slightly boosted the PPV for the muon identification at one point.
This is also reflected in the efficiencies for identifying particles which virtually take the same form as the one from the univariate Bayesian approach.

Differences in abundances were not observed. The global efficiencies over all particle species are displayed in~\autoref{tab:overall_efficiencies}. The drop between

\begin{table}[ht]
	\centering
	\begin{tabular}{l|l}
		Particle identification approach & Overall identification efficiency \\
		\hline
		legacy PID & $0.848342$ \\
		pidProbability & $0.793748$ \\
		simple Bayes & $0.884109$ \\
		univariate Bayes & $0.886597$ \\
		multivariate Bayes & $0.886819$
	\end{tabular}
	\caption{Overall efficiencies for various particle identical approaches.}
	\label{tab:overall_efficiencies}
\end{table}

\section{Neural network approach}
\label{sec:neural_network_approach}

\subsection{Design}
\label{subsec:neural_network_design}

The neural network used in the following discussion consists of eight sequential layers. By comparing a multitude of networks with different number of layers eight was found to perform best without just over-fitting the data. Hereby two different concepts for layers were used; On one hand a so called \textit{dense} layer connects all inputs with each node in the layers, as seen in~\autoref{fig:sample_neural_network_design}. It usually has a rather high number of free parameters depending on the function used in each node. On the other hand a so called \textit{dropout} layer was employed which as its name suggests drops a certain percentage of inputs by chance and enforces the training on only the remaining nodes. It is commonly used to counteract the effect of having a lot of free parameters as it randomly disregards values and therefore possibly drops nodes which would otherwise start to over-fit (see~\cite{MachineLearning:DeepLearning}).

Three different approaches for the choice of input parameters were evaluated. First the unaltered pidProbability variable for each detector was used as a baseline. As second approach a rich mixture of variables were used, containing information on the momentum, the angle between the beampipe and the track, the distance of the vertex to the interaction point, the energy, the curvature of the track, the charge, the legacy PIDs as well as the pidProbability for each detector. The third approach used the same initial inputs as the second one. However prior to passing the values to the network, a Principal Component Analysis (see~\cite{BigDataManagementAndAnalytics:TextProcessingAndHigh-DimensionalData}) on the standardized data was performed. The standardization step centers the data by removing the mean and scales the the value so that the standard deviation becomes one. It is an important pre-processing step as it counteracts the effect of having different units and scales for each variable.

The output, i.e. the actual classification, was done in the final step of the network via the previously discussed softmax algorithm using the gradient of the cross-entropy for weight adaption. In order to get a unique classification, the class with the highest softmax value was selected. As targets for classification the six long-living particle species were used with the addition of a zeroth class used for classifying particles which do not belong to the six other classes. This zeroth class includes just a fractional amount of particles but it is important to have it either way as each track on which the network is supposed to learn is required to have a target associated with it.

\autoref{tab:neural_network_parameters} shows the parametrization used for the network. Overall it has between $1,200$ and $1,800$ free parameters which are to be adapted, depending on the dimensionality of the input.

\begin{table}[ht]
	\centering
	\begin{tabular}{l|llll}
		Layer number & Type & Activation & Bias & \#Outputs  \\
		\hline
		1 & Dense & ReLU & True & 14 \\
		2 & Dropout ($20\%$) \\
		3 & Dense & ReLU & True & 21 \\
		4 & Dropout ($20\%$) \\
		5 & Dense & ReLU & True & 14 \\
		6 & Dropout ($20\%$) \\
		7 & Dense & ReLU & True & 10 \\
		8 & Dense & Softmax & True & 7
	\end{tabular}
	\caption{Design parameters of the neural network.}
	\label{tab:neural_network_parameters}
\end{table}

\subsection{Performance}
\label{subsec:neural_network_performance}
